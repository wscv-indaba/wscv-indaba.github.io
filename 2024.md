---
layout: 2024
buttons:
  Invited Speakers: '#invited-speakers'
  Program: '#program'
  Call for Papers: '#call-for-papers'
  Organizers: '#organizers'
  Important Dates: '#important-dates'
---

<!-- # Overview -->
<div class="workshopdesc">
Since the acquisition and annotation of real-world data is complex, computer vision datasets only capture a fraction of our continuous world. To cope with unseen conditions, fight biases of the training data, or simply reduce our dependency on data, algorithms may be trained in a weak-/un-supervised fashion. Recently, novel avenues of research have emerged to relax supervision (less labels, less data) for example using multimodal models, generative AI, transfer learning, continual learning, etc. This lets us foresee new frontiers of computer vision, holding immense potential for the African society.

<b>This 3rd WSCV edition will gather leading computer vision figures with keynotes and lightning talks</b> on various topics like: 
zero-shot training, multimodal models / foundational models, open-vocabulary, self-/un-supervised training, diffusion models, robustness and uncertainty estimation; as well as talks on african initiatives.
<br>
<br>


<!-- <div style="text-align: center;">
<a href="#call-for-papers" style="display: inline-block; padding: 10px; margin-left: auto; margin-right: auto; background-color: red; color: white; font-weight: bolder; border-radius: 10px;">Submit your work üìú (Deadline: August 20th)</a>
</div> -->
<span style="color: red;">
üì¢ The workshop will have a poster session showcasing participants works on computer vision. <b><br>Best papers will be awarded with a prize üèÜ</b><br/>
<br/>
News 07/26: Submissions are open check the <a href="#call-for-papers">submission instructions</a>.
<br>
<!-- * News 08/20: Submissions are closed.<br>
* News 08/14: Paper submission deadline is extended to August 20th.<br>
* News 08/01: Submission website is open.-->
</span>
<br> 
<!-- <em>Remote access details will be added here close to the event.</em><br>

 <div class="live">
  The recording is available:<br>
  <a href="https://www.youtube.com/watch?v=12bSyGYJkgA" target="_blank">Workshop on Weakly Supervised Computer-Vision</a>
 </div>
 -->
</div>

# Invited Speakers
<div class="speakers">
  <div class="speaker">
    <a href="https://vicky.kalogeiton.info" target="_blank">
    <img src="/assets/imgs/people/vicky_kalogeiton.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Vicky Kalogeiton</a><br>
    √âcole Polytechnique
  </div>
  <div class="speaker">
    <a href="https://danielomeiza.github.io" target="_blank">
    <img src="/assets/imgs/people/daniel_omeiza.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Daniel Omeiza</a><br>
    University of Oxford
  </div>
  <div class="speaker">
    <a href="https://sites.google.com/view/jnabende/home" target="_blank">
    <img src="/assets/imgs/people/joyce_nakatumba-nabende.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Joyce Nakatumba-Nabende</a><br>
    Makerere University
  </div>
  <div class="speaker">
    <a href="https://osimeoni.github.io/" target="_blank">
    <img src="/assets/imgs/people/oriane_simeoni.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Oriane Sim√©oni</a><br>
    valeo.ai
  </div>
  <div class="speaker">
    <a href="https://danielajisafe.github.io/" target="_blank">
    <img src="/assets/imgs/people/daniel_ajisafe.png" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Daniel Ajisafe</a><br>
    University of British Columbia
  </div>
  <div class="speaker">
    <a href="https://pierlui92.github.io/" target="_blank">
    <img src="/assets/imgs/people/pierluigi_ramirez.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Pierluigi Zama Ramirez</a><br>
    University of Bologna
  </div>
</div>

## Program

TBD

<!-- <div class="program">
<div class="entry start">
  <span style="color: rgb(200, 0, 0); font-weight: bold;">Times are local Ghana time (GMT).</span><br>
  <div class="time">11:00am</div> - <b>Workshop start</b>
</div>
<div class="entry">
  <div class="speaker">
  </div>
  <div class="details">
    <div class="time">11:00am</div> - Opening remarks. <a href="https://team.inria.fr/rits/membres/raoul-de-charette/" target="_blank">Raoul de Charette</a>, Inria<br>
  </div>
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://stelat.eu/" target="_blank">
    <img alt="St√©phane Lathuili√®re" src="/assets/imgs/people/stephane_lathuiliere.jpg">
    <br>
    </a>
  </div>
  <div class="details">
    <div class="time">11:10am</div> - <a href="https://stelat.eu/" target="_blank">St√©phane Lathuili√®re</a>, Telecom Paris<br>
    <div class="title">Data Frugality in Image Generation, Image Generation for Data Frugality</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">In this talk, we aim to showcase recent developments that demonstrate how image generation tasks can now be addressed with only a few examples. We'll explain how image synthesis models conditioned on semantic maps can be trained effectively using a small set of samples through a transfer learning approach. Next, we'll explore how recent text-conditioned diffusion models go even further, enabling semantic image synthesis in a zero-shot manner. <br>Furthermore, we'll delve into the potential of deep image-generation techniques to facilitate learning in perception tasks with minimal training data. Specifically, we'll investigate how a semantic segmentation model can be adapted from one visual domain to another using just a single unlabeled sample. This adaptation is achieved through leveraging pre-trained text-to-image models. While earlier methods relied on style transfer for such adaptations, we'll illustrate how text-to-image diffusion models can generate synthetic target datasets that replicate real-world scenes and styles. This method allows guiding image generation towards specific concepts while retaining spatial context from a single training image.</div>
  </div>
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://www.mohamed-elhoseiny.com/" target="_blank">
    <img alt="Mohamed H. Elhoseiny" src="/assets/imgs/people/mohamed-h-elhoseiny.jpg">
    <br>
    </a>
  </div>
  <div class="details">
    <div class="time">12:00pm</div> - <a href="https://www.mohamed-elhoseiny.com/" target="_blank">Mohamed H. Elhoseiny</a>, KAUST<br>
    <div class="title">Imaginative Vision Language Models: Towards human-level imaginative AI skills transforming species discovery, content creation, self-driving cars, and Emotional Health / Health Care</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">Most existing AI learning methods can be categorized into supervised, semi-supervised, and unsupervised methods. These approaches rely on defining empirical risks or losses on the provided labeled and/or unlabeled data. Beyond extracting learning signals from labeled/unlabeled training data, we will reflect in this talk on a class of methods that can learn beyond the vocabulary that was trained on and can compose or create novel concepts. Specifically, we address the question of how these AI skills may assist species discovery, content creation, self-driving cars, emotional health, and more. We refer to this class of techniques as imagination AI methods, and we will dive into how we developed several approaches to build machine learning methods that can See, Create, Drive, and Feel. See: recognize unseen visual concepts by imaginative learning signals and how that may extend in a continual setting where seen and unseen classes change dynamically. Create: generate novel art and fashion by creativity losses. Drive: improve trajectory forecasting for autonomous driving by modeling hallucinative driving intents. Feel: generate emotional descriptions of visual art that are metaphoric and go beyond grounded descriptions. Feel: generate emotional descriptions of visual art that are metaphoric and go beyond grounded descriptions, and how to build these AI systems to be more inclusive of multiple cultures. I will also conclude by pointing out future directions where imaginative AI may help develop better assistive technology for multicultural and more inclusive metaverse, emotional health, and drug discovery.</div>
  </div>
</div>
<div class="entry lunch">
  üçΩÔ∏è Lunch break üçΩÔ∏è
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://scholar.google.com/citations?user=eiB0s-kAAAAJ" target="_blank">
    <img alt="Mathilde Caron" src="/assets/imgs/people/mathilde_caron.jpg">
    <br>
    </a>
  </div>
  <div class="details">
    <div class="time">2:00pm</div> - <a href="https://scholar.google.com/citations?user=eiB0s-kAAAAJ" target="_blank">Mathilde Caron</a>, Google<br>
    <div class="title">Large-scale and Efficient Visual Understanding with Transformers</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">I will present various advances that we have made in developing accurate and scalable models for visual and multi-modal understanding, requiring few annotations. My presentation will include:<br>
    <a href="https://arxiv.org/abs/2104.14294" target="_blank">DINO</a>: We expose our observations from adapting self-supervised learning to vision transformer:  first, self-supervised ViT features contain explicit information about the semantic segmentation of an image. Second, these features can be easily adapted to downstream task with minimal adaptation and are for instance excellent k-NN classifiers.<br>
    <a href="https://arxiv.org/abs/2304.06708" target="_blank">Verbs in action</a>: Current video models and benchmarks have a single frame/object/noun bias. We tackle this problem by bringing together a set of benchmarks that focus more on "verb" or "temporal" understanding and use LLMs to create harder text pairs for contrastive pretraining that forces video models to pay more attention to verbs. <br>
    <a href="https://arxiv.org/abs/2306.07196" target="_blank">RECO</a>: We propose to equip existing foundation models with the ability to refine their embedding with cross-modal retrieved information from an external memory at inference time, which greatly improves zero-shot predictions on fine-grained recognition tasks.</div>
  </div>
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://people.epfl.ch/mathieu.salzmann" target="_blank">
    <img alt="Mathieu Salzmann" src="/assets/imgs/people/mathieu_salzmann.jpg">
    <br>
    </a>
  </div>
  <div class="details">
    <div class="time">2:50pm</div> - <a href="https://people.epfl.ch/mathieu.salzmann" target="_blank">Mathieu Salzmann</a>, EPFL<br>
    <div class="title">Generalizing to Unseen Objects without Re-training</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">In many practical situations, the objects that will be observed when deploying a deep learning model may differ from those seen during training. This is for example the case in automated driving, where obstacles on the road can be of any kind, significantly differing from the standard categories used to train an object detector. This also occurs in the context of space debris capture, which relies on estimating the 6D pose of debris that may differ from the training ones. In such situations, no annotations are provided for the new objects and re-training or fine-tuning the model is often not practical. Nevertheless, such unseen objects must be handled to ensure operation safety. In this talk, I will present our recent progress towards training deep learning models able to generalize to new object categories at test time. I will focus on the scenarios of road obstacle detection and 6D object pose estimation, and will show that, in both cases, generalization can be facilitated by learning to compare images.</div>
  </div>
</div>
<div class="entry spotlight">
  <div class="speaker">
  </div>
  <div class="details">
    <div class="time">3:40pm</div>
    <div class="title">- Spotlights presentations</div><br>
    <ul class="papers">
      <li><span class="title">Mirror-Aware Neural Humans</span> <span class="authors">Daniel Ajisafe, James Tang, Shih-Yang Su, Bastian Wandt, Helge Rhodin</span></li>
      <li><span class="title">COVID-Attention: Efficient COVID19 Detection using Pre-trained Deep Models based on Vision Transformers and X-ray Images</span> <span class="authors">Imed-eddine Haouli, Walid Hariri, Hassina Seridi-Bouchelaghem</span></li>
      <li><span class="title">3D reconstructions of brain from MRI scans using neural radiance fields</span> <span class="authors">Khadija Iddrisu, Sylwia Malec, Alessandro Crimi</span></li>
      <li><span class="title">Mobile-Based Early Skin Disease Diagnosis for Melanin-Rich Skins</span> <span class="authors">Mathews Jahnical Jere</span></li>
      <li><span class="title">Hybrid Optimization of Coccidiosis Chicken Disease Prediction, Detection and Prevention Using Deep Learning Frameworks</span> <span class="authors">David Wairimu</span></li>
    </ul>
  </div>
</div>
<div class="entry poster">
  <div class="speaker">
  </div>
  <div class="details">
    <div class="time">4:00pm</div>
    <div class="title">üëÅÔ∏èüó£Ô∏èü§ùüèæ Poster sessions (15 posters) + ‚òï Coffee Break</div><br>
  </div>
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://imagine.enpc.fr/~varolg/" target="_blank">
    <img alt="G√ºl Varol" src="/assets/imgs/people/gul_varol.jpg">
    <br>
    </a>
  </div>
  <div class="details">
    <div class="time">4:50pm</div> - <a href="https://imagine.enpc.fr/~varolg/" target="_blank">G√ºl Varol</a>, √âcole des Ponts<br>
    <div class="title">Automatic annotation of open-vocabulary sign language videos</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">Research on sign language technologies has suffered from the lack of data to train machine learning models. This talk will describe our recent efforts on scalable approaches to automatically annotate continuous sign language videos with the goal of building a large-scale dataset. In particular, we leverage weakly-aligned subtitles from sign interpreted broadcast footage. These subtitles provide us candidate keywords to search and localise individual signs. To this end, we develop several sign spotting techniques: (i) using mouthing cues at the lip region, (ii) looking up videos from sign language dictionaries, and (iii) exploring the sign localisation that emerges from the attention mechanism of a sequence prediction model. We further tackle the subtitle alignment problem to improve their synchronization with signing. With these methods, we build the BBC-Oxford British Sign Language Dataset (BOBSL), continuous signing videos of more than a thousand hours, containing millions of sign instance annotations from a large vocabulary. More information about the dataset can be found at <a href="https://arxiv.org/abs/2111.03635" target="_blank">https://arxiv.org/abs/2111.03635</a></div>
  </div>
</div>
<div class="entry awards">
  <div class="speaker">
  </div>
  <div class="details">
    <div class="time">5:40pm</div>
    <div class="title">- Panel Closing Remarks + üèÜ Announcement of the Awards üèÜ</div><br>
  </div>
</div>
<div class="entry end">
  <div class="time">6:00pm</div> - <b>Workshop end</b>
</div>
</div> -->

## Call for Papers



<div style="text-align: justify">
We welcome submission of short/regular papers on any computer vision topics, for presentation at the poster session.
It can be original or recently published work.<br>
<br>

<div style="text-align: center;">
<span style="/*text-decoration: line-through;*/ color: red;"><b>Submission deadline: August 11<sup>th</sup> 2024 (Anywhere on Earth).</b></span><br>
<!-- <span style="color: red;"><b>Submission deadline (EXTENSION): August 20<sup>th</sup> 2023 (Anywhere on Earth).</b></span><br> -->
<b>üèÜ&nbsp;Best papers will be awarded with a prize.&nbsp;üèÜ</b><br>
 (prize pool: 900‚Ç¨)
</div>

<b>Submission website</b>: <br>
<div style="margin-left: 1em">
  <a href="https://openreview.net/group?id=DeepLearningIndaba.com/2024/Workshop/WSCV" target="_blank">https://openreview.net/group?id=DeepLearningIndaba.com/2024/Workshop/WSCV</a>
</div><br>
<!--
<span style="color:  red;">Please submit pdf of your work on CMT: <a href="https://cmt3.research.microsoft.com/WSCV2023/" target="_blank">https://cmt3.research.microsoft.com/WSCV2023/</a><br>
<b>Deadline is extended to August 7<sup>th</sup> (11:59pm AOE).</b></span><br>
//-->
<b>Instructions</b>:<br>
<div style="margin-left: 1em;">
Submissions should be 4 to 8 pages (excluding references pages).<br>
We encourage submissions to use <a href="assets/latex/WSCV.zip" target="_blank">our double-column latex kit</a> but we will accept single/double columns submissions with any format. Anonymity is optional.<br>
We accept submission which are original, under review, or already published.<br>
<br>
‚ö†Ô∏è If you don't yet have an openreview account, note that the account creation can take a few days for validation. Create your account asap.
</div>
<br>
<br>
The topics of interest include, but are not limited to:

  <ul>
    <li>3D computer vision</li>
    <li>Adversarial learning, adversarial attack for vision algorithms</li>
    <li>Autonomous agents with vision (reinforcement/imitation learning)</li>
    <li>Biometrics, face, gesture, body pose</li>
    <li>Computational photography, image and video synthesis</li>   
    <li>Explainable, fair, accountable, privacy-preserving, ethical computer vision</li>
    <li>Foundation models, multimodal large language models, etc.</li>
    <li>Image recognition and understanding (object detection, categorization, segmentation, scene modeling, visual reasoning)</li>
    <li>Low-level and physics-based vision</li>
    <li>Semi-/Self-/Un-supervised learning and Few-/Zero-shot algorithms</li>
    <li>Transfer learning (domain adaptation, etc.)</li>
    <li>Video understanding (tracking, action recognition, etc.)</li>
    <li>Multi-modal vision (image+text, image+sound, etc.)</li>
  </ul>
</div>

<!-- <span style="color: red;">************ ADD PDF online ************</span> -->
<!-- <a href="https://drive.google.com/file/d/1ktqInynvEBldBYn-bg9SfZXjU5EWb-L7/view?usp=sharing" target="_blank">PDF version</a> //-->

## Organizers
<div class="organizers e2023">
  <div class="organizer">
    <a target="_blank" href="https://team.inria.fr/rits/membres/raoul-de-charette/">
    <img alt="Raoul de Charette" src="/assets/imgs/people/raoul_de-charette.png" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Raoul de Charette</a><br>
    Inria
  </div>

  <div class="organizer">
    <a target="_blank" href="https://fabvio.github.io/">
    <img alt="Fabio Pizzati" src="/assets/imgs/people/fabio_pizzati.png" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Fabio Pizzati</a><br>
    Oxford Uni.
  </div>

  <div class="organizer">
    <a target="_blank" href="https://tuanhungvu.github.io/">
    <img alt="Tuan-Hung Vu" src="/assets/imgs/people/tuan-hung_vu.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Tuan-Hung Vu</a><br>
    Valeo.ai
  </div>

  <div class="organizer">
    <a target="_blank" href="https://abursuc.github.io/">
    <img alt="Andrei Bursuc" src="/assets/imgs/people/andrei_bursuc.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Andrei Bursuc</a><br>
    Valeo.ai
  </div>
  <div class="organizer">
    <a target="_blank" href="https://sites.google.com/site/sileyeoba/home">
    <img src="/assets/imgs/people/sileye_ba.png" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Sileye Ba</a><br>
    L'Or√©al
  </div>

</div>

<h3>Volunteers</h3>
Like in previous editions, volunteers are welcome. Just contact us.

## Important dates
- Submission deadline: <strong>August 11, 2024 (AOE)</strong>.
- Decision notification: TBD
- Workshop date: <strong>September 6, 2024.</strong><br>
<br>

üì¢ Want to volunteer ? Any questions ? Contact <a href="https://team.inria.fr/rits/membres/raoul-de-charette/">Raoul de Charette</a>.


<hr>
#### Thanks to our award sponsors
<div class="sponsors">
  <div class="sponsor">
    <img src="assets/imgs/sponsor_gopro.png">  <a href="https://www.gopro.com" target="_blank">GoPro, Be a Hero</a><br />
    <img src="assets/imgs/snap.png" style="width: 13%;">  <a href="https://www.snap.com" target="_blank">Snap, the fastest way to share a moment</a>
  </div>
</div>
