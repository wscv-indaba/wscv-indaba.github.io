---
layout: 2024
buttons:
  Invited Speakers: '#invited-speakers'
  Program: '#program'
  Call for Papers: '#call-for-papers'
  Organizers: '#organizers'
  Important Dates: '#important-dates'
---

<!-- # Overview -->
<div class="workshopdesc">
Since the acquisition and annotation of real-world data is complex, computer vision datasets only capture a fraction of our continuous world. To cope with unseen conditions, fight biases of the training data, or simply reduce our dependency on data, algorithms may be trained in a weak-/un-supervised fashion. Recently, novel avenues of research have emerged to relax supervision (less labels, less data) for example using multimodal models, generative AI, transfer learning, continual learning, etc. This lets us foresee new frontiers of computer vision, holding immense potential for the African society.

<b>This 3rd WSCV edition will gather leading computer vision figures with keynotes and lightning talks</b> on various topics like: 
zero-shot training, multimodal models / foundational models, open-vocabulary, self-/un-supervised training, diffusion models, robustness and uncertainty estimation; as well as talks on african initiatives.
<br>
<br>


<!-- <div style="text-align: center;">
<a href="#call-for-papers" style="display: inline-block; padding: 10px; margin-left: auto; margin-right: auto; background-color: red; color: white; font-weight: bolder; border-radius: 10px;">Submit your work üìú (Deadline: August 20th)</a>
</div> -->
<span style="color: red;">
üì¢ The workshop will have a poster session showcasing participants works on computer vision. <b><br>Prize will be awarded üèÜ</b><br/>
<br/>
<!-- News 07/26: Submissions are open check the <a href="#call-for-papers">submission instructions</a>.<br>
News 08/11: Deadline extended to <b>August 20th</b>.<br> -->
News 08/21: Papers submission is now closed. DLI participants can still present their DLI poster at the workshop (cf. contact at the bottom). We announced the <a href="#call-for-papers">prizes details</a> ;-)<br>
<!-- * * News 08/01: Submission website is open.-->
</span>
<br> 
<!-- <em>Remote access details will be added here close to the event.</em><br>

 <div class="live">
  The recording is available:<br>
  <a href="https://www.youtube.com/watch?v=12bSyGYJkgA" target="_blank">Workshop on Weakly Supervised Computer-Vision</a>
 </div>
 -->
</div>

# Invited Speakers
<div class="speakers">
  <div class="speaker">
    <a href="https://vicky.kalogeiton.info" target="_blank">
    <img src="/assets/imgs/people/vicky_kalogeiton.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Vicky Kalogeiton</a><br>
    √âcole Polytechnique
  </div>
  <div class="speaker">
    <a href="https://danielomeiza.github.io" target="_blank">
    <img src="/assets/imgs/people/daniel_omeiza.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Daniel Omeiza</a><br>
    University of Oxford
  </div>
  <div class="speaker">
    <a href="https://sites.google.com/view/jnabende/home" target="_blank">
    <img src="/assets/imgs/people/joyce_nakatumba-nabende.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Joyce Nakatumba-Nabende</a><br>
    Makerere University
  </div>
  <div class="speaker">
    <a href="https://osimeoni.github.io/" target="_blank">
    <img src="/assets/imgs/people/oriane_simeoni.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Oriane Sim√©oni</a><br>
    valeo.ai
  </div>
  <div class="speaker">
    <a href="http://candaceross.io/" target="_blank">
    <img src="/assets/imgs/people/candace_ross.png" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Candace Ross</a><br>
    Meta AI
  </div>
  <div class="speaker">
    <a href="https://danielajisafe.github.io/" target="_blank">
    <img src="/assets/imgs/people/daniel_ajisafe.png" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Daniel Ajisafe</a><br>
    University of British Columbia
  </div>
  <div class="speaker">
    <a href="https://pierlui92.github.io/" target="_blank">
    <img src="/assets/imgs/people/pierluigi_ramirez.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Pierluigi Zama Ramirez</a><br>
    University of Bologna
  </div>
</div>

## Program

<div class="program">
<div class="entry start">
  <span style="color: rgb(200, 0, 0); font-weight: bold;">Times are local Dakar time (GMT).</span><br>
  <div class="time">08:30am</div> - <b>Workshop start</b>
</div>
<div class="entry">
  <div class="speaker">
  </div>
  <div class="details">
    <div class="time"></div> - Opening remarks. <a href="https://team.inria.fr/rits/membres/raoul-de-charette/" target="_blank">Raoul de Charette</a>, Inria<br>
  </div>
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://osimeoni.github.io/" target="_blank">
    <img src="/assets/imgs/people/oriane_simeoni.jpg">
    <br>
    </a>
  </div>
  <div class="details">
    <div class="time">08:30am</div> - <a href="https://osimeoni.github.io/" target="_blank">Oriane Sim√©oni</a>, Valeo.ai<br>
    <div class="title">Object localization (almost) for free harnessing self-supervised features</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">The localization of objects in images is today at the heart of many perception systems. However, training object detectors requires large and expensive campaigns of annotation for a finite and pre-defined vocabulary. Instead, being able to discover objects in images without knowing in advance which objects populate a dataset is an exciting prospect. In this talk we will discuss solutions to exploit self-supervised pre-trained features to perform class-agnostic object localization with zero annotation, and such without requiring object proposals nor expensive exploration of image collections. Then, we will  investigate means to unite unsupervised object localization with VLM open-vocabulary features leading to good quality open-vocabulary semantic segmentation with no extra annotation.</div>
  </div>
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://pierlui92.github.io/" target="_blank">
    <img src="/assets/imgs/people/pierluigi_ramirez.jpg">
    <br>
    </a>
  </div>
  <div class="details">
    <div class="time">09:30am</div> - <a href="https://pierlui92.github.io/" target="_blank">Pierluigi Zama Ramirez</a>, University of Bologna<br>
    <div class="title">Neural Processing of 3D Neural Fields</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">In recent years, Neural Fields have emerged as an effective tool for encoding diverse continuous signals such as images, videos, and 3D shapes. However, given that Neural Fields are essentially neural networks, it remains unclear whether and how they can be seamlessly integrated into deep learning pipelines for solving downstream tasks. This presentation delves into the novel research problem of Neural Fields processing through deep learning pipelines, exploring techniques  for leveraging this data representation to perform tasks such as classification, segmentation, and even more complex tasks like natural language understanding of the neural field content.</div>
  </div>
</div>
<div class="entry">
  <div class="speaker">
  </div>
  <div class="details">
    <div class="time">10:30am</div>
    <div class="title">‚òï Coffee Break</div><br>
  </div>
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://danielajisafe.github.io/" target="_blank">
    <img src="/assets/imgs/people/daniel_ajisafe.png">
    <br>
    </a>
  </div>
  <div class="details">
    <div class="time">11:00am</div> - <a href="https://danielajisafe.github.io/" target="_blank">Daniel Ajisafe</a>, University of British Columbia<br>
    <div class="title">Behind the Scenes - Learning Human Body Pose, Shape and Appearance from Mirror Videos</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">Humans exists as essential part of the world and its important to develop algorithms that can reconstruct humans in their full digital form. While prior works have attempted using marker suits to collect 3D data or using multiple cameras to achieve this purpose, mirrors form an affordable and available alternative producing the reflection of a person in a way that is temporally synchronized. In this talk, I will uncover whats behind the scenes, specifically our main contributions that extends articulated neural radiance fields to include a notion of the mirror and making it sample efficient over potential occlusion regions. I will also demonstrate the benefit of learning a complete body model from mirror scenes.</div>
  </div>
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://sites.google.com/view/jnabende/home" target="_blank">
    <img src="/assets/imgs/people/joyce_nakatumba-nabende.jpg">
    <br>
    </a>
  </div>
  <div class="details">
    <div class="time">11:30am</div> - <a href="https://sites.google.com/view/jnabende/home" target="_blank">Joyce Nakatumba-Nabende</a>, Makerere AI Lab<br>
    <div class="title">(TBD) Computer vision and African Initiatives at Makerere AI Lab</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">TBD</div>
  </div>
</div>
<div class="entry">
  <div class="speaker">
    <a href="http://candaceross.io/" target="_blank">
    <img src="/assets/imgs/people/candace_ross.png">
    <br>
    </a>
  </div>
  <div class="details">
    <div class="time">12:00pm</div> - <a href="http://candaceross.io/" target="_blank">Candace Ross</a>, Meta AI<br>
    <div class="title">(TBD) Vision and Language</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">TBD</div>
  </div>
</div>
<div class="entry lunch">
  üçΩÔ∏è Lunch break üçΩÔ∏è
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://danielomeiza.github.io/" target="_blank">
    <img src="/assets/imgs/people/daniel_omeiza.jpg">
    <br>
    </a>
  </div>
  <div class="details">
    <div class="time">2:00pm</div> - <a href="https://danielomeiza.github.io/" target="_blank">Daniel Omeiza</a>, University of Oxford<br>
    <div class="title">Providing Explanations for Responsible Autonomous Driving</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">TBD</div>
  </div>
</div>
<div class="entry spotlight">
  <div class="speaker">
  </div>
  <div class="details">
    <div class="time">2:45pm</div>
    <div class="title">- Spotlights presentations</div><br>
    <ul class="papers">
      <li><span class="title">BioNAS: Incorporating Bio-inspired Learning Rules to Neural Architecture Search</span> <span class="authors">Imane Hamzaoui</span></li>
      <li><span class="title">RGB UAV Imagery Segmentation: Comparative Study</span> <span class="authors">Mathews Jahnical Jere</span></li>
      <li><span class="title">AJA-pose: A Framework for Animal Pose Estimation based on VHR Network Architecture</span> <span class="authors">Austin Kaburia Kibaara, Joan Kabura, Antony M. Gitau, Ciira wa Maina</span></li>
    </ul>
  </div>
</div>
<div class="entry poster">
  <div class="speaker">
  </div>
  <div class="details">
    <div class="time">3:00pm</div>
    <div class="title">üëÅÔ∏èüó£Ô∏èü§ùüèæ Poster sessions (20 posters) + ‚òï Coffee Break</div><br>
  </div>
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://vicky.kalogeiton.info/" target="_blank">
    <img src="/assets/imgs/people/vicky_kalogeiton.jpg">
    <br>
    </a>
  </div>
  <div class="details">
    <div class="time">4:00pm</div> - <a href="https://vicky.kalogeiton.info/" target="_blank">Vicky Kalogeiton</a>, √âcole Polytechnique<br>
    <div class="title">Multimodality for story-level understanding and generation of visual data</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">In this talk, I will address the importance of multimodality (i.e. using more than one modality, such as video, audio, text, masks and clinical data) for story-level recognition and generation. First, I will focus on story-level multimodal video understanding, as audio, faces, and visual temporal structure come naturally with the videos, and we can exploit them for free (<a href="https://link.springer.com/content/pdf/10.1007/s11263-024-02000-2.pdf" target="_blank">FunnyNet-W</a> and <a href="https://arxiv.org/pdf/2406.10221" target="_blank">Short Film Dataset</a>). Then, I will show some examples of visual generation from text and other modalities (<a href="https://arxiv.org/pdf/2407.01516" target="_blank">ET</a>, <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Dufour_Dont_Drop_Your_Samples_Coherence-Aware_Training_Benefits_Conditional_Diffusion_CVPR_2024_paper.pdf" target="_blank">CAD</a>, <a href="https://arxiv.org/pdf/2404.13040" target="_blank">DynamicGuidance</a>). 
  </div>
  </div>
</div>
<div class="entry awards">
  <div class="speaker">
  </div>
  <div class="details">
    <div class="time">5:00pm</div>
    <div class="title">- Panel + üèÜ Announcement of the Awards üèÜ</div><br>
  </div>
</div>
<div class="entry end">
  <div class="time">5:30pm</div> - <b>Workshop end</b>
</div>
</div>

## Call for Papers



<div style="text-align: justify">
We welcome submission of short/regular papers on any computer vision topics, for presentation at the poster session.
It can be original or recently published work.<br>
<br>

<div style="text-align: center;">
<span style="text-decoration: line-through; color: red;"><b>Submission deadline: August 11<sup>th</sup> 2024 (Anywhere on Earth).</b></span><br>
<span style="color: red; text-decoration: line-through;"><b>Submission deadline (EXTENSION): August 20<sup>th</sup> 2024 (Anywhere on Earth).</b></span><br>
<b>üèÜ&nbsp;Prizes will be awarded.&nbsp;üèÜ<br>
500$ cash (Snap gift)<br>
GoPro HERO12 (GoPro gift)</b>
</div>

<b>Submission website</b>: <br>
<div style="margin-left: 1em">
  <a href="https://openreview.net/group?id=DeepLearningIndaba.com/2024/Workshop/WSCV" target="_blank">https://openreview.net/group?id=DeepLearningIndaba.com/2024/Workshop/WSCV</a>
</div><br>
<!--
<span style="color:  red;">Please submit pdf of your work on CMT: <a href="https://cmt3.research.microsoft.com/WSCV2023/" target="_blank">https://cmt3.research.microsoft.com/WSCV2023/</a><br>
<b>Deadline is extended to August 7<sup>th</sup> (11:59pm AOE).</b></span><br>
//-->
<b>Instructions</b>:<br>
<div style="margin-left: 1em;">
Submissions should be 4 to 8 pages (excluding references pages).<br>
We encourage submissions to use <a href="assets/latex/WSCV.zip" target="_blank">our double-column latex kit</a> but we will accept single/double columns submissions with any format. Anonymity is optional.<br>
We accept submission which are original, under review, or already published.<br>
<br>
‚ö†Ô∏è If you don't yet have an openreview account, note that the account creation can take a few days for validation. Create your account asap.
</div>
<br>
<br>
The topics of interest include, but are not limited to:

  <ul>
    <li>3D computer vision</li>
    <li>Adversarial learning, adversarial attack for vision algorithms</li>
    <li>Autonomous agents with vision (reinforcement/imitation learning)</li>
    <li>Biometrics, face, gesture, body pose</li>
    <li>Computational photography, image and video synthesis</li>   
    <li>Explainable, fair, accountable, privacy-preserving, ethical computer vision</li>
    <li>Foundation models, multimodal large language models, etc.</li>
    <li>Image recognition and understanding (object detection, categorization, segmentation, scene modeling, visual reasoning)</li>
    <li>Low-level and physics-based vision</li>
    <li>Semi-/Self-/Un-supervised learning and Few-/Zero-shot algorithms</li>
    <li>Transfer learning (domain adaptation, etc.)</li>
    <li>Video understanding (tracking, action recognition, etc.)</li>
    <li>Multi-modal vision (image+text, image+sound, etc.)</li>
  </ul>
</div>

<!-- <span style="color: red;">************ ADD PDF online ************</span> -->
<!-- <a href="https://drive.google.com/file/d/1ktqInynvEBldBYn-bg9SfZXjU5EWb-L7/view?usp=sharing" target="_blank">PDF version</a> //-->

## Organizers
<div class="organizers e2023">
  <div class="organizer">
    <a target="_blank" href="https://team.inria.fr/rits/membres/raoul-de-charette/">
    <img alt="Raoul de Charette" src="/assets/imgs/people/raoul_de-charette.png" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Raoul de Charette</a><br>
    Inria
  </div>

  <div class="organizer">
    <a target="_blank" href="https://fabvio.github.io/">
    <img alt="Fabio Pizzati" src="/assets/imgs/people/fabio_pizzati.png" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Fabio Pizzati</a><br>
    Oxford Uni.
  </div>

  <div class="organizer">
    <a target="_blank" href="https://tuanhungvu.github.io/">
    <img alt="Tuan-Hung Vu" src="/assets/imgs/people/tuan-hung_vu.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Tuan-Hung Vu</a><br>
    Valeo.ai
  </div>

  <div class="organizer">
    <a target="_blank" href="https://abursuc.github.io/">
    <img alt="Andrei Bursuc" src="/assets/imgs/people/andrei_bursuc.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Andrei Bursuc</a><br>
    Valeo.ai
  </div>
  <div class="organizer">
    <a target="_blank" href="https://sites.google.com/site/sileyeoba/home">
    <img src="/assets/imgs/people/sileye_ba.png" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Sileye Ba</a><br>
    L'Or√©al
  </div>

</div>

<h3>Volunteers</h3>
<div class="volunteers e2023">
  <div class="volunteer">
    <a href="https://www.linkedin.com/in/lama-moukheiber/" target="_blank"><img alt="Lama Moukheiber" src="/assets/imgs/people/lama_moukheiber.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Lama Moukheiber</a><br>
    University at Buffalo
  </div>
  <div class="volunteer">
    <a href="https://ug.linkedin.com/in/benjamin-rukundo-539ab01a6" target="_blank">
    <img alt="Benjamin Rukundo" src="/assets/imgs/people/benjamin_rukundo.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Benjamin Rukundo</a><br>
    Makerere University
  </div>
  <div class="volunteer">
    <a href="https://www.linkedin.com/in/fatimabintibrahim" target="_blank">
    <img src="/assets/imgs/people/fatima_ibrahim.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Fatima-Bint Ibrahim</a><br>
  </div>
  <div class="volunteer">
    <a href="https://www.linkedin.com/in/loyani-loyani-568102143" target="_blank">
    <img src="/assets/imgs/people/loyani_loyani_kisula.jpg" style ="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;">
    <br>
    Loyani Loyani Kisula</a><br>
    NM-AIST
  </div>

</div>
Volunteers are welcome to help at the workshop. Just contact us.

## Important dates
- Submission deadline: <s>August 11, 2024 (AOE)</s> <b>August 20, 2024 (AOE)</b>.
- Decision notification: August 23, 2024.
- Workshop date: <strong>September 6, 2024.</strong><br>
<br>

üì¢ Want to volunteer ? Any questions ? Contact <a href="https://team.inria.fr/rits/membres/raoul-de-charette/">Raoul de Charette</a>.


<hr>
#### Thanks to our award sponsors
<div class="sponsors">
  <div class="sponsor">
    <img src="assets/imgs/sponsor_gopro.png">  <a href="https://www.gopro.com" target="_blank">GoPro, Be a Hero</a><br />
    <img src="assets/imgs/snap.png" style="width: 13%;">  <a href="https://www.snap.com" target="_blank">Snap, the fastest way to share a moment</a>
  </div>
</div>
