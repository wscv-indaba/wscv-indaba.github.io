<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Weakly Supervised Computer Vision</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Weakly Supervised Computer Vision" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/2023.html" />
<meta property="og:url" content="http://localhost:4000/2023.html" />
<meta property="og:site_name" content="Weakly Supervised Computer Vision" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Weakly Supervised Computer Vision" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","headline":"Weakly Supervised Computer Vision","url":"http://localhost:4000/2023.html"}</script>
<!-- End Jekyll SEO tag -->

    <meta property="og:title" content='INDABA'/>
    <meta property="og:image:type" content="image/jpeg">
    <meta property="og:image:width" content="200">
    <meta property="og:image:height" content="200">
    <meta property="og:type" content='website'/>
    <meta name="description" content=""/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#731578">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="/assets/css/style.css?v=c6938fb4febcc3180ef8355b2d9d30dbe9704803">
    
    <meta name="google-site-verification" content="EaOBAFJS2oZAhYIqcnG58x5_a5rmawbEfR2m7fBQRSk" />

    
  </head>
  <body>

    <section class="page-header page-header2023">
      <!-- <img alt="logo" src="pics/placehold-logo.svg" class = "logo" > -->
      <div class="head">
      <h1 class="project-name" style="color:#ffffff; text-shadow: -1px -1px 0 #880000, 1px -1px 0 #880000, -1px 1px 0 #880000, 1px 1px 0 #880000;">Weakly Supervised Computer Vision</h1><br>
     <h2 class="project-tagline" style="color:#ffffff ;  opacity: 100%; text-shadow: -1px -1px 0 #000000, 1px -1px 0 #000000, -1px 1px 0 #000000, 1px 1px 0 #000000;">
      <span>workshop at the <a href='https://deeplearningindaba.com/2023/' target='_blank' style='text-decoration: underline; text-shadow: none;'>Deep Learning Indaba</a><br>
      8<sup>th</sup> September, 2023<br>
      Accra, Ghana
      </span>
      </h2>
    </div>

    <div class="top-menu">
    
      <a href="#invited-speakers" class="btn">Invited Speakers</a>
    
      <a href="#program" class="btn">Program</a>
    
      <a href="#call-for-papers" class="btn">Call for Papers</a>
    
      <a href="#organizers" class="btn">Organizers</a>
    
      <a href="#important-dates" class="btn">Important Dates</a>
    
      <div class="btn dropdown">Other editions
        <div class="list">
          
            <a href="/2023">2023</a>
          
            <a href="/2022">2022</a>
          
        </div>
      </div>
    </div>

    </section>
    
   
    <section class="main-content">
      <!-- # Overview -->
<div class="workshopdesc">
To understand scenes from images, video or 3D data, computer vision often relies on models trained on large datasets. But as the field matured, algorithms are now expected to perform in real-world, beyond training conditions. Hence, new tasks emerged like generalization, open-world vision which seeks to adjust to unseen conditions (lighting, weather, etc.), robustness to adversarial attacks, image generation, etc. Weakly-supervised learning helps address these challenges because it relaxes the need of costly annotations and minimizes the biases of training datasets. Thus, truly paving the way to real-world applications like autonomous driving, mobile robotics, virtual reality, image generation...<br />
<b>This workshop will deep dive into the latest research with talks from renowned speakers.</b> Among others, we will address techniques like vision-language model (VLM), transfer learning, diffusion models, contrastive learning, vision transformers (ViT), continual learning, neural fields, and else; while revolving on how to relax supervision (less labels, less data), adapt to unseen data, or benefit from other modalities (text, image + text, video + text).
<br />
<br />


<!-- <div style="text-align: center;">
<a href="#call-for-papers" style="display: inline-block; padding: 10px; margin-left: auto; margin-right: auto; background-color: red; color: white; font-weight: bolder; border-radius: 10px;">Submit your work üìú (Deadline: August 20th)</a>
</div> -->
<span style="color: red;">
üì¢ The workshop will have a poster session showcasing participants works on computer vision. <b>Best papers will be awarded with a prize üèÜ</b><br />
* News 08/28: Decisions are out on <a href="https://openreview.net/group?id=DeepLearningIndaba.com/2023/Workshop/WSCV" target="_blank">OpenReview</a>.<br />
<!-- * News 08/20: Submissions are closed.<br>
* News 08/14: Paper submission deadline is extended to August 20th.<br>
* News 08/01: Submission website is open.-->
</span>
<br /> 
<!-- <em>Remote access details will be added here close to the event.</em><br>

 <div class="live">
  The recording is available:<br>
  <a href="https://www.youtube.com/watch?v=12bSyGYJkgA" target="_blank">Workshop on Weakly Supervised Computer-Vision</a>
 </div>
 -->
</div>

<h1 id="invited-speakers">Invited Speakers</h1>
<div class="speakers">
  <div class="speaker">
    <a href="https://people.epfl.ch/mathieu.salzmann" target="_blank">
    <img alt="Mathieu Salzmann" src="/assets/imgs/people/mathieu_salzmann.jpg" style="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;" />
    <br />
    Mathieu Salzmann</a><br />
    EPFL
  </div>
  <div class="speaker">
    <a href="https://imagine.enpc.fr/~varolg/" target="_blank">
    <img alt="St√©phane Lathuili√®re" src="/assets/imgs/people/gul_varol.jpg" style="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;" />
    <br />
    G√ºl Varol</a><br />
    √âcole des Ponts
  </div>
  <div class="speaker">
    <a href="https://stelat.eu/" target="_blank">
    <img alt="St√©phane Lathuili√®re" src="/assets/imgs/people/stephane_lathuiliere.jpg" style="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;" />
    <br />
    St√©phane Lathuili√®re</a><br />
    Telecom Paris
  </div>
  <div class="speaker">
    <a href="https://scholar.google.com/citations?user=eiB0s-kAAAAJ" target="_blank">
    <img alt="Mathilde Caron" src="/assets/imgs/people/mathilde_caron.jpg" style="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;" />
    <br />
    Mathilde Caron</a><br />
    Google
  </div>
  <div class="speaker">
    <a href="https://www.mohamed-elhoseiny.com" target="_blank">
    <img alt="Mohamed H. Elhoseiny" src="/assets/imgs/people/mohamed-h-elhoseiny.jpg" style="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;" />
    <br />
    Mohamed H. Elhoseiny</a><br />
    KAUST
  </div>
</div>

<h2 id="program">Program</h2>
<div class="program">
<div class="entry start">
  <span style="color: rgb(200, 0, 0); font-weight: bold;">Times are local Ghana time (GMT).</span><br />
  <div class="time">11:00am</div> - <b>Workshop start</b>
</div>
<div class="entry">
  <div class="speaker">
  </div>
  <div class="details">
    <div class="time">11:00am</div> - Opening remarks. <a href="https://team.inria.fr/rits/membres/raoul-de-charette/" target="_blank">Raoul de Charette</a>, Inria<br />
  </div>
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://stelat.eu/" target="_blank">
    <img alt="St√©phane Lathuili√®re" src="/assets/imgs/people/stephane_lathuiliere.jpg" />
    <br />
    </a>
  </div>
  <div class="details">
    <div class="time">11:10am</div> - <a href="https://stelat.eu/" target="_blank">St√©phane Lathuili√®re</a>, Telecom Paris<br />
    <div class="title">Data Frugality in Image Generation, Image Generation for Data Frugality</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">In this talk, we aim to showcase recent developments that demonstrate how image generation tasks can now be addressed with only a few examples. We'll explain how image synthesis models conditioned on semantic maps can be trained effectively using a small set of samples through a transfer learning approach. Next, we'll explore how recent text-conditioned diffusion models go even further, enabling semantic image synthesis in a zero-shot manner. <br />Furthermore, we'll delve into the potential of deep image-generation techniques to facilitate learning in perception tasks with minimal training data. Specifically, we'll investigate how a semantic segmentation model can be adapted from one visual domain to another using just a single unlabeled sample. This adaptation is achieved through leveraging pre-trained text-to-image models. While earlier methods relied on style transfer for such adaptations, we'll illustrate how text-to-image diffusion models can generate synthetic target datasets that replicate real-world scenes and styles. This method allows guiding image generation towards specific concepts while retaining spatial context from a single training image.</div>
  </div>
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://www.mohamed-elhoseiny.com/" target="_blank">
    <img alt="Mohamed H. Elhoseiny" src="/assets/imgs/people/mohamed-h-elhoseiny.jpg" />
    <br />
    </a>
  </div>
  <div class="details">
    <div class="time">12:00pm</div> - <a href="https://www.mohamed-elhoseiny.com/" target="_blank">Mohamed H. Elhoseiny</a>, KAUST<br />
    <div class="title">Imaginative Vision Language Models: Towards human-level imaginative AI skills transforming species discovery, content creation, self-driving cars, and Emotional Health / Health Care</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">Most existing AI learning methods can be categorized into supervised, semi-supervised, and unsupervised methods. These approaches rely on defining empirical risks or losses on the provided labeled and/or unlabeled data. Beyond extracting learning signals from labeled/unlabeled training data, we will reflect in this talk on a class of methods that can learn beyond the vocabulary that was trained on and can compose or create novel concepts. Specifically, we address the question of how these AI skills may assist species discovery, content creation, self-driving cars, emotional health, and more. We refer to this class of techniques as imagination AI methods, and we will dive into how we developed several approaches to build machine learning methods that can See, Create, Drive, and Feel. See: recognize unseen visual concepts by imaginative learning signals and how that may extend in a continual setting where seen and unseen classes change dynamically. Create: generate novel art and fashion by creativity losses. Drive: improve trajectory forecasting for autonomous driving by modeling hallucinative driving intents. Feel: generate emotional descriptions of visual art that are metaphoric and go beyond grounded descriptions. Feel: generate emotional descriptions of visual art that are metaphoric and go beyond grounded descriptions, and how to build these AI systems to be more inclusive of multiple cultures. I will also conclude by pointing out future directions where imaginative AI may help develop better assistive technology for multicultural and more inclusive metaverse, emotional health, and drug discovery.</div>
  </div>
</div>
<div class="entry lunch">
  üçΩÔ∏è Lunch break üçΩÔ∏è
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://scholar.google.com/citations?user=eiB0s-kAAAAJ" target="_blank">
    <img alt="Mathilde Caron" src="/assets/imgs/people/mathilde_caron.jpg" />
    <br />
    </a>
  </div>
  <div class="details">
    <div class="time">2:00pm</div> - <a href="https://scholar.google.com/citations?user=eiB0s-kAAAAJ" target="_blank">Mathilde Caron</a>, Google<br />
    <div class="title">Large-scale and Efficient Visual Understanding with Transformers</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">I will present various advances that we have made in developing accurate and scalable models for visual and multi-modal understanding, requiring few annotations. My presentation will include:<br />
    <a href="https://arxiv.org/abs/2104.14294" target="_blank">DINO</a>: We expose our observations from adapting self-supervised learning to vision transformer:  first, self-supervised ViT features contain explicit information about the semantic segmentation of an image. Second, these features can be easily adapted to downstream task with minimal adaptation and are for instance excellent k-NN classifiers.<br />
    <a href="https://arxiv.org/abs/2304.06708" target="_blank">Verbs in action</a>: Current video models and benchmarks have a single frame/object/noun bias. We tackle this problem by bringing together a set of benchmarks that focus more on "verb" or "temporal" understanding and use LLMs to create harder text pairs for contrastive pretraining that forces video models to pay more attention to verbs. <br />
    <a href="https://arxiv.org/abs/2306.07196" target="_blank">RECO</a>: We propose to equip existing foundation models with the ability to refine their embedding with cross-modal retrieved information from an external memory at inference time, which greatly improves zero-shot predictions on fine-grained recognition tasks.</div>
  </div>
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://people.epfl.ch/mathieu.salzmann" target="_blank">
    <img alt="Mathieu Salzmann" src="/assets/imgs/people/mathieu_salzmann.jpg" />
    <br />
    </a>
  </div>
  <div class="details">
    <div class="time">2:50pm</div> - <a href="https://people.epfl.ch/mathieu.salzmann" target="_blank">Mathieu Salzmann</a>, EPFL<br />
    <div class="title">Generalizing to Unseen Objects without Re-training</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">In many practical situations, the objects that will be observed when deploying a deep learning model may differ from those seen during training. This is for example the case in automated driving, where obstacles on the road can be of any kind, significantly differing from the standard categories used to train an object detector. This also occurs in the context of space debris capture, which relies on estimating the 6D pose of debris that may differ from the training ones. In such situations, no annotations are provided for the new objects and re-training or fine-tuning the model is often not practical. Nevertheless, such unseen objects must be handled to ensure operation safety. In this talk, I will present our recent progress towards training deep learning models able to generalize to new object categories at test time. I will focus on the scenarios of road obstacle detection and 6D object pose estimation, and will show that, in both cases, generalization can be facilitated by learning to compare images.</div>
  </div>
</div>
<div class="entry spotlight">
  <div class="speaker">
  </div>
  <div class="details">
    <div class="time">3:40pm</div>
    <div class="title">- Spotlights presentations</div><br />
    <ul class="papers">
      <li><span class="title">Mirror-Aware Neural Humans</span> <span class="authors">Daniel Ajisafe, James Tang, Shih-Yang Su, Bastian Wandt, Helge Rhodin</span></li>
      <li><span class="title">COVID-Attention: Efficient COVID19 Detection using Pre-trained Deep Models based on Vision Transformers and X-ray Images</span> <span class="authors">Imed-eddine Haouli, Walid Hariri, Hassina Seridi-Bouchelaghem</span></li>
      <li><span class="title">3D reconstructions of brain from MRI scans using neural radiance fields</span> <span class="authors">Khadija Iddrisu, Sylwia Malec, Alessandro Crimi</span></li>
      <li><span class="title">Mobile-Based Early Skin Disease Diagnosis for Melanin-Rich Skins</span> <span class="authors">Mathews Jahnical Jere</span></li>
      <li><span class="title">Hybrid Optimization of Coccidiosis Chicken Disease Prediction, Detection and Prevention Using Deep Learning Frameworks</span> <span class="authors">David Wairimu</span></li>
    </ul>
  </div>
</div>
<div class="entry poster">
  <div class="speaker">
  </div>
  <div class="details">
    <div class="time">4:00pm</div>
    <div class="title">üëÅÔ∏èüó£Ô∏èü§ùüèæ Poster sessions + ‚òï Coffee Break</div><br />
  </div>
</div>
<div class="entry">
  <div class="speaker">
    <a href="https://imagine.enpc.fr/~varolg/" target="_blank">
    <img alt="G√ºl Varol" src="/assets/imgs/people/gul_varol.jpg" />
    <br />
    </a>
  </div>
  <div class="details">
    <div class="time">4:50pm</div> - <a href="https://imagine.enpc.fr/~varolg/" target="_blank">G√ºl Varol</a>, √âcole des Ponts<br />
    <div class="title">Automatic annotation of open-vocabulary sign language videos</div>
    <a onclick="this.parentElement.getElementsByClassName('abstract')[0].style.display=this.parentElement.getElementsByClassName('abstract')[0].style.display!='block' ? 'block' : 'none';">[+] Abstract</a>
    <div class="abstract">Research on sign language technologies has suffered from the lack of data to train machine learning models. This talk will describe our recent efforts on scalable approaches to automatically annotate continuous sign language videos with the goal of building a large-scale dataset. In particular, we leverage weakly-aligned subtitles from sign interpreted broadcast footage. These subtitles provide us candidate keywords to search and localise individual signs. To this end, we develop several sign spotting techniques: (i) using mouthing cues at the lip region, (ii) looking up videos from sign language dictionaries, and (iii) exploring the sign localisation that emerges from the attention mechanism of a sequence prediction model. We further tackle the subtitle alignment problem to improve their synchronization with signing. With these methods, we build the BBC-Oxford British Sign Language Dataset (BOBSL), continuous signing videos of more than a thousand hours, containing millions of sign instance annotations from a large vocabulary. More information about the dataset can be found at <a href="https://arxiv.org/abs/2111.03635" target="_blank">https://arxiv.org/abs/2111.03635</a></div>
  </div>
</div>
<div class="entry awards">
  <div class="speaker">
  </div>
  <div class="details">
    <div class="time">5:40pm</div>
    <div class="title">- Panel Closing Remarks + üèÜ Announcement of the Awards üèÜ</div><br />
  </div>
</div>
<div class="entry end">
  <div class="time">6:00pm</div> - <b>Workshop end</b>
</div>
</div>

<h2 id="call-for-papers">Call for Papers</h2>

<div style="text-align: justify">
We welcome submission of short/regular papers on ANY computer vision topics, for presentation at the poster session.
It can be original or recently published work.<br />
<br />

<div style="text-align: center;">
<span style="text-decoration: line-through;"><b>Submission deadline: August 13<sup>th</sup> 2023 (Anywhere on Earth).</b></span><br />
<span style="color: red;"><b>Submission deadline (EXTENSION): August 20<sup>th</sup> 2023 (Anywhere on Earth).</b></span><br />
<b>üèÜ&nbsp;Best papers will be awarded with a prize.&nbsp;üèÜ</b><br />1000‚Ç¨ prize pool (GoPro, Nvidia)<br /><br />
</div>

<b>Submission website</b>: <br />
<div style="margin-left: 1em">
  <a href="https://openreview.net/group?id=DeepLearningIndaba.com/2023/Workshop/WSCV" target="_blank">https://openreview.net/group?id=DeepLearningIndaba.com/2023/Workshop/WSCV</a>
</div><br />
<!--
<span style="color:  red;">Please submit pdf of your work on CMT: <a href="https://cmt3.research.microsoft.com/WSCV2023/" target="_blank">https://cmt3.research.microsoft.com/WSCV2023/</a><br>
<b>Deadline is extended to August 7<sup>th</sup> (11:59pm AOE).</b></span><br>
//-->
<b>Instructions</b>:<br />
<div style="margin-left: 1em;">
Submissions should be 4 to 8 pages (excluding references pages).<br />
We encourage submissions to use <a href="assets/latex/WSCV2023.zip" target="_blank">our double-column latex kit</a> but we will accept single/double columns submissions with any format. Anonymity is optional.<br />
We accept submission which are original, under review, or already published.
</div>
<br />
<br />
The topics of interest include, but are not limited to:

  <ul>
    <li>3D computer vision</li>
    <li>Adversarial learning, adversarial attack for vision algorithms</li>
    <li>Autonomous agents with vision (reinforcement/imitation learning)</li>
    <li>Biometrics, face, gesture, body pose</li>
    <li>Computational photography, image and video synthesis</li>   
    <li>Explainable, fair, accountable, privacy-preserving, ethical computer vision</li>
    <li>Image recognition and understanding (object detection, categorization, segmentation, scene modeling, visual reasoning)</li>
    <li>Low-level and physics-based vision</li>
    <li>Semi-/Self-/Un-supervised learning and Few-/Zero-shot algorithms</li>
    <li>Transfer learning (domain adaptation, etc.)</li>
    <li>Video understanding (tracking, action recognition, etc.)</li>
    <li>Multi-modal vision (image+text, image+sound, etc.)</li>
  </ul>
</div>

<!-- <span style="color: red;">************ ADD PDF online ************</span> -->
<!-- <a href="https://drive.google.com/file/d/1ktqInynvEBldBYn-bg9SfZXjU5EWb-L7/view?usp=sharing" target="_blank">PDF version</a> //-->

<h2 id="organizers">Organizers</h2>
<div class="organizers e2023">
  <div class="organizer">
    <a target="_blank" href="https://team.inria.fr/rits/membres/raoul-de-charette/">
    <img alt="Raoul de Charette" src="/assets/imgs/people/raoul_de-charette.png" style="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;" />
    <br />
    Raoul de Charette</a><br />
    Inria
  </div>

  <div class="organizer">
    <a target="_blank" href="https://fabvio.github.io/">
    <img alt="Fabio Pizzati" src="/assets/imgs/people/fabio_pizzati.png" style="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;" />
    <br />
    Fabio Pizzati</a><br />
    Oxford Uni.
  </div>

  <div class="organizer">
    <a target="_blank" href="https://ptrckprz.github.io/">
    <img alt="Patrick P√©rez" src="/assets/imgs/people/patrick_perez.jpg" style="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;" />
    <br />
    Patrick P√©rez</a><br />
    Valeo.ai
  </div>

  <div class="organizer">
    <a target="_blank" href="https://tuanhungvu.github.io/">
    <img alt="Tuan-Hung Vu" src="/assets/imgs/people/tuan-hung_vu.jpg" style="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;" />
    <br />
    Tuan-Hung Vu</a><br />
    Valeo.ai
  </div>

  <div class="organizer">
    <a target="_blank" href="https://abursuc.github.io/">
    <img alt="Andrei Bursuc" src="/assets/imgs/people/andrei_bursuc.jpg" style="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;" />
    <br />
    Andrei Bursuc</a><br />
    Valeo.ai
  </div>
</div>

<h3>Volunteers</h3>
<div class="volunteers e2023">
  <div class="volunteer">
    <a href="https://www.linkedin.com/in/abdulhaq-adetunji-salako-9452b4122" target="_blank">
    <img alt="Abdulhaq Adetunji Salako" src="/assets/imgs/people/abdulhaq_adetunji_salako.jpg" style="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;" />
    <br />
    Abdulhaq Adetunji Salako</a><br />
    Research fellow
  </div>
  <div class="volunteer">
    <img alt="Imane Hamzaoui" src="/assets/imgs/people/imane_hamzaoui.png" style="border-radius: 50%; object-fit: cover; width = 100%; aspect-ratio: 1;" />
    <br />
    Imane Hamzaoui<br />
    ESI
  </div>
</div>

<h2 id="important-dates">Important dates</h2>
<ul>
  <li>Submission deadline: <s>August 13, 2023 (AOE).</s> <strong>August 20, 2023 (AOE).</strong></li>
  <li>Decision notification: <s>August 25, 2023.</s> <strong>August 27, 2023</strong></li>
  <li>Workshop date: <strong>September 8, 2023.</strong><br />
<br /></li>
</ul>

<p>üì¢ Want to volunteer ? Any questions ? Contact <a href="https://team.inria.fr/rits/membres/raoul-de-charette/">Raoul de Charette</a>.</p>

<hr />

<h4 id="thanks-to-our-award-sponsors">Thanks to our award sponsors</h4>
<div class="sponsors">
  <div class="sponsor">
    <img src="assets/imgs/sponsor_nvidia.png" /> 
    <div class="headline"><a href="https://developer.nvidia.com/developer-program/?ncid=ref-dev-171762-inria" target="_blank">Sign up to the NVIDIA Developer Program</a> to start your developer journey now.</div>
  </div>
  <div class="sponsor">
    <img src="assets/imgs/sponsor_gopro.png" />  <a href="https://www.gopro.com" target="_blank">GoPro, Be a Hero</a>
  </div>
</div>


      <footer class="site-footer">
       
      </footer>
    </section>

    
      <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'G-0CG00X9EN0', 'auto');
        ga('send', 'pageview');
      </script>
    
  </body>
</html>
